{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46d52b1",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "   * Dataset used : [Sentiment140 dataset](https://www.kaggle.com/kazanova/sentiment140).  It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .\n",
    "   \n",
    "   * We have tried to imitate the model architecture described in [this paper](https://www.itm-conferences.org/articles/itmconf/abs/2021/02/itmconf_icitsd2021_01012/itmconf_icitsd2021_01012.html).\n",
    "   \n",
    "   * This model will be used in further analysis of Twitter User Data to generate a measure of thier trust and affluence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd5caf",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80bb02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For working with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Removing stopwords and Stemming\n",
    "import nltk\n",
    "\n",
    "# Time taken by functions\n",
    "import time\n",
    "\n",
    "# Cleaning Tweets\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba21d3b",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "* Reading the Train CSV file\n",
    "* Dropping a few useless columns\n",
    "* Mapping Positive (4) to 1 and Negative (0) to 0\n",
    "* Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5795497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Train CSV file\n",
    "\n",
    "raw_data = pd.read_csv(\"./train.csv\" , header = None , encoding = 'latin')\n",
    "\n",
    "# Dropping a few useless columns\n",
    "\n",
    "raw_data.columns = ['Y', 'A', 'B', 'C', 'D', 'X']\n",
    "raw_data = raw_data.drop(['A', 'B', 'C', 'D'], axis=1)\n",
    "\n",
    "# Mapping Positive (4) to 1 and Negative (0) to 0\n",
    "\n",
    "raw_data['Y'] = raw_data['Y'].map({4 : 1 , 0 : 0})\n",
    "raw_data['Y'].value_counts()\n",
    "\n",
    "# Shuffling the data\n",
    "\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52e12b",
   "metadata": {},
   "source": [
    "## Preprocessing the Tweets\n",
    "* Cleaning the Tweets\n",
    "    * Lowercasing\n",
    "    * Removing all the urls, user tags, hashtags and some punctuations.\n",
    "    * Stemming and removing spaces\n",
    "* Train-Test Split\n",
    "* Removing very rare words ( <= 60 occurences ).\n",
    "* Splitting into Targets and Features.\n",
    "* Count Vectorizing and TF-IDF Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97379eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763.4864540100098\n"
     ]
    }
   ],
   "source": [
    "# Declaring the function\n",
    "def process_text(text):\n",
    "\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replacing all the urls\n",
    "    text = re.sub('(?i)\\\\b((?:https?://|www\\\\d{0,3}[.]|[a-z0-9.\\\\-]+[.][a-z]{2,4}/)(?:[^\\\\s()<>]+|\\\\(([^\\\\s()<>]+|(\\\\([^\\\\s()<>]+\\\\)))*\\\\))+(?:\\\\(([^\\\\s()<>]+|(\\\\([^\\\\s()<>]+\\\\)))*\\\\)|[^\\\\s`!()\\\\[\\\\]{};:\\'\\\\\".,<>?\\xc2\\xab\\xc2\\xbb\\xe2\\x80\\x9c\\xe2\\x80\\x9d\\xe2\\x80\\x98\\xe2\\x80\\x99]))'\n",
    "               , '', text)\n",
    "\n",
    "    # Replacing all user tags\n",
    "    text = re.sub(r\"@[^\\s]+\", '', text)\n",
    "\n",
    "    # Replacing all hashtags\n",
    "    text = re.sub(r\"#[^\\s]+\", '', text)\n",
    "\n",
    "    # Remove some punctuations\n",
    "    text = re.sub(r\"[!?,'\\\"*)@#%(&$_.^-]\", '', text)\n",
    "\n",
    "    # Splitting on spaces\n",
    "    text = text.split(' ')\n",
    "\n",
    "    # Stemming and removing spaces\n",
    "    stemmer_ps = nltk.stem.PorterStemmer()  \n",
    "    text = [stemmer_ps.stem(word) for word in text if len(word)]\n",
    "\n",
    "    return text\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "raw_data['X'] = raw_data['X'].map(process_text)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a931ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train, Test = train_test_split(raw_data , stratify = raw_data[\"Y\"], test_size=0.02)\n",
    "\n",
    "Train = Train.to_numpy()\n",
    "Test = Test.to_numpy()\n",
    "\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d17af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.388003587722778\n"
     ]
    }
   ],
   "source": [
    "# Creating the word frequency dictionary to remove very rare words.\n",
    "wordfreq = {}\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for _ , text in Train:\n",
    "    for w in text:\n",
    "        if w not in wordfreq:\n",
    "            wordfreq[w] = 1\n",
    "    else:\n",
    "        wordfreq[w] += 1\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b922ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398352 2439 99.39145340089972 0.6085465991002792\n"
     ]
    }
   ],
   "source": [
    "# Getting words that have occured more than 75 times in tweets\n",
    "\n",
    "words = [word for word in wordfreq if wordfreq[word] <= 60]\n",
    "low_words = len(words)\n",
    "total_words = len(wordfreq)\n",
    "print(low_words , total_words - low_words , (low_words / total_words * 100) , ((total_words - low_words) / total_words * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad92913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the low occurence words from the wordfreq dictionaries\n",
    "\n",
    "for word in words:\n",
    "    if word in wordfreq: \n",
    "        del wordfreq[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110ca44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.084949731826782\n"
     ]
    }
   ],
   "source": [
    "# Removing the low occurence words from the tweets\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(Train)):\n",
    "    Train[i][1] = [ word for word in Train[i][1] if word in wordfreq]\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7434c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3390800952911377\n"
     ]
    }
   ],
   "source": [
    "# Joining for Vectorizer\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(Train)):\n",
    "    Train[i][1] = \" \".join(Train[i][1])\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4cac630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the targets and labels for train\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(len(Train)):\n",
    "    if ( len(Train[i][1]) > 2 ):\n",
    "        X.append(Train[i][1])\n",
    "        Y.append(Train[i][0])\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6cca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectoritzing \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_CV = CountVectorizer()\n",
    "\n",
    "X_CV = vectorizer_CV.fit_transform(X)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_TFIDF = TfidfVectorizer()\n",
    "\n",
    "X_TFIDF = vectorizer_TFIDF.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a736d",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f716b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the models\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fc87a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the models \n",
    "\n",
    "# Logistic Regression\n",
    "lr_CV = LogisticRegression(random_state=0 , max_iter=1000)\n",
    "lr_TFIDF = LogisticRegression(random_state=0 , max_iter=1000)\n",
    "\n",
    "# Random Forests\n",
    "rf_CV = RandomForestClassifier(n_estimators=200 , max_depth=2, random_state=0)\n",
    "rf_TFIDF = RandomForestClassifier(n_estimators=200 , max_depth=2, random_state=0)\n",
    "\n",
    "# Gradient Boosting \n",
    "gb_CV = GradientBoostingClassifier(n_estimators=20, max_depth=2, random_state=0)\n",
    "gb_TFIDF = GradientBoostingClassifier(n_estimators=20, max_depth=2, random_state=0)\n",
    "\n",
    "# ADABoost\n",
    "adb_CV = AdaBoostClassifier(n_estimators=20, random_state=0)\n",
    "adb_TFIDF = AdaBoostClassifier(n_estimators=20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bddf09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression started!\n",
      "Logistic regression ended in 100.65697360038757 secs\n",
      "Random Forests started!\n",
      "Random Forests ended in 95.21197438240051 secs\n",
      "GB started!\n",
      "GB ended in 123.39501428604126 secs\n",
      "ADB started!\n",
      "ADB ended in 90.28889489173889 secs\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression started!\")\n",
    "start = time.time()\n",
    "clf_lr_CV = lr_CV.fit(X_CV, Y)\n",
    "end = time.time()\n",
    "print(\"Logistic regression ended in \" + str(end - start) + \" secs\")\n",
    "\n",
    "print(\"Random Forests started!\")\n",
    "start = time.time()\n",
    "clf_rf_CV = rf_CV.fit(X_CV, Y)\n",
    "end = time.time()\n",
    "print(\"Random Forests ended in \" + str(end - start) + \" secs\")\n",
    "\n",
    "print(\"GB started!\")\n",
    "start = time.time()\n",
    "clf_gb_CV = gb_CV.fit(X_CV, Y)\n",
    "end = time.time()\n",
    "print(\"GB ended in \" + str(end - start) + \" secs\")\n",
    "\n",
    "print(\"ADB started!\")\n",
    "start = time.time()\n",
    "clf_adb_CV = adb_CV.fit(X_CV, Y)\n",
    "end = time.time()\n",
    "print(\"ADB ended in \" + str(end - start) + \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc56dc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression started!\n",
      "Logistic regression ended in 52.55265784263611 secs\n",
      "Random Forests started!\n",
      "Random Forests ended in 105.24345660209656 secs\n",
      "GB started!\n",
      "GB ended in 225.63196730613708 secs\n",
      "ADB started!\n",
      "ADB ended in 162.3167896270752 secs\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression started!\")\n",
    "start = time.time()\n",
    "clf_lr_TFIDF = lr_TFIDF.fit(X_TFIDF, Y)\n",
    "end = time.time()\n",
    "print(\"Logistic regression ended in \" + str(end - start) + \" secs\")\n",
    "\n",
    "print(\"Random Forests started!\")\n",
    "start = time.time()\n",
    "clf_rf_TFIDF = rf_TFIDF.fit(X_TFIDF, Y)\n",
    "end = time.time()\n",
    "print(\"Random Forests ended in \" + str(end - start) + \" secs\")\n",
    "\n",
    "print(\"GB started!\")\n",
    "start = time.time()\n",
    "clf_gb_TFIDF = gb_TFIDF.fit(X_TFIDF, Y)\n",
    "end = time.time()\n",
    "print(\"GB ended in \" + str(end - start) + \" secs\")\n",
    "\n",
    "print(\"ADB started!\")\n",
    "start = time.time()\n",
    "clf_adb_TFIDF = adb_TFIDF.fit(X_TFIDF, Y)\n",
    "end = time.time()\n",
    "print(\"ADB ended in \" + str(end - start) + \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc008645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VC_CV started!\n",
      "VC_CV ended in 377.9691722393036 secs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "# Count Vectorized Part\n",
    "VC_CV_LR = LogisticRegression(random_state=0 , max_iter=1000)\n",
    "VC_CV_GB = GradientBoostingClassifier(n_estimators=20, max_depth=2, random_state=0)\n",
    "VC_CV_AB = AdaBoostClassifier(n_estimators=20, random_state=0)\n",
    "VC_CV_RF = RandomForestClassifier(n_estimators=200 , max_depth=2, random_state=0)\n",
    "\n",
    "VC_CV = VotingClassifier(estimators=[\n",
    "    ('lr_cv', VC_CV_LR), \n",
    "    ('gb_cv', VC_CV_GB), \n",
    "    ('rf_cv', VC_CV_RF ), \n",
    "    ('ab_cv', VC_CV_AB) \n",
    "], voting='soft')\n",
    "\n",
    "print(\"VC_CV started!\")\n",
    "start = time.time()\n",
    "VC_CV = VC_CV.fit(X_CV, Y)\n",
    "end = time.time()\n",
    "print(\"VC_CV ended in \" + str(end - start) + \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e36217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VC_TFIDF started!\n",
      "VC_TFIDF ended in 508.7841799259186 secs\n"
     ]
    }
   ],
   "source": [
    "# TFIDF Vectorized Part\n",
    "VC_TFIDF_LR = LogisticRegression(random_state=0 , max_iter=1000)\n",
    "VC_TFIDF_GB = GradientBoostingClassifier(n_estimators=20, max_depth=2, random_state=0)\n",
    "VC_TFIDF_AB = AdaBoostClassifier(n_estimators=20, random_state=0)\n",
    "VC_TFIDF_RF = RandomForestClassifier(n_estimators=200 , max_depth=2, random_state=0)\n",
    "\n",
    "VC_TFIDF = VotingClassifier(estimators=[\n",
    "    ('lr_tfidf', VC_TFIDF_LR), \n",
    "    ('gb_tfidf', VC_TFIDF_GB), \n",
    "    ('rf_tfidf', VC_TFIDF_RF ), \n",
    "    ('ab_tfidf', VC_TFIDF_AB) \n",
    "], voting='soft')\n",
    "\n",
    "print(\"VC_TFIDF started!\")\n",
    "start = time.time()\n",
    "VC_TFIDF = VC_TFIDF.fit(X_TFIDF, Y)\n",
    "end = time.time()\n",
    "print(\"VC_TFIDF ended in \" + str(end - start) + \" secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307551d",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17dd710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the test set\n",
    "\n",
    "for i in range(len(Test)):\n",
    "    Test[i][1] = [ word for word in Test[i][1] if word in wordfreq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fa36420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the targets and labels for train\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for i in range(len(Test)):\n",
    "    X_test.append(\" \".join(Test[i][1]))\n",
    "    Y_test.append(Test[i][0])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f9db70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing\n",
    "\n",
    "X_CV_test = vectorizer_CV.transform(X_test)\n",
    "X_TFIDF_test = vectorizer_TFIDF.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c18e2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting for single models\n",
    "\n",
    "Y_pred_lr_CV = clf_lr_CV.predict(X_CV_test)\n",
    "Y_pred_rf_CV = clf_rf_CV.predict(X_CV_test)\n",
    "Y_pred_gb_CV = clf_gb_CV.predict(X_CV_test)\n",
    "Y_pred_adb_CV = clf_adb_CV.predict(X_CV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa7d1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_lr_TFIDF = clf_lr_TFIDF.predict(X_TFIDF_test)\n",
    "Y_pred_rf_TFIDF = clf_rf_TFIDF.predict(X_TFIDF_test)\n",
    "Y_pred_gb_TFIDF = clf_gb_TFIDF.predict(X_TFIDF_test)\n",
    "Y_pred_adb_TFIDF = clf_adb_TFIDF.predict(X_TFIDF_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c8023e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VC_CV_pred = VC_CV.predict(X_CV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f94e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "VC_TFIDF_pred = VC_TFIDF.predict(X_TFIDF_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b024be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f9e028b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics (model_name , Y_test , Y_pred ):\n",
    "    print(\"------ Metrics for \" + model_name + \"------\")\n",
    "    print(\"Accuracy : \" + str(accuracy_score(Y_test, Y_pred)))\n",
    "    print(\"Precision : \" + str(precision_score(Y_test, Y_pred)))\n",
    "    print(\"Recall : \" + str(recall_score(Y_test, Y_pred)))\n",
    "    print(\"F1 Score : \" + str(f1_score(Y_test, Y_pred)))\n",
    "    print(\"------------------------------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ca46e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Metrics for Logistic Regression------\n",
      "Accuracy : 0.7798125\n",
      "Precision : 0.7661079410366144\n",
      "Recall : 0.8055625\n",
      "F1 Score : 0.7853399951255179\n",
      "------------------------------------------- \n",
      "------ Metrics for Random Forest------\n",
      "Accuracy : 0.71059375\n",
      "Precision : 0.7112671640855226\n",
      "Recall : 0.709\n",
      "F1 Score : 0.7101317725124418\n",
      "------------------------------------------- \n",
      "------ Metrics for Gradient Boosting------\n",
      "Accuracy : 0.6515\n",
      "Precision : 0.6154835636017151\n",
      "Recall : 0.8074375\n",
      "F1 Score : 0.6985131116517977\n",
      "------------------------------------------- \n",
      "------ Metrics for ADABoost------\n",
      "Accuracy : 0.64775\n",
      "Precision : 0.6045925139368198\n",
      "Recall : 0.8540625\n",
      "F1 Score : 0.7079944044350034\n",
      "------------------------------------------- \n"
     ]
    }
   ],
   "source": [
    "metrics(\"Logistic Regression\", Y_test ,Y_pred_lr_CV)\n",
    "metrics(\"Random Forest\", Y_test ,Y_pred_rf_CV,)\n",
    "metrics(\"Gradient Boosting\", Y_test ,Y_pred_gb_CV)\n",
    "metrics(\"ADABoost\", Y_test ,Y_pred_adb_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f2277b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Metrics for Logistic Regression------\n",
      "Accuracy : 0.781\n",
      "Precision : 0.7731138379297777\n",
      "Recall : 0.7954375\n",
      "F1 Score : 0.7841168135050213\n",
      "------------------------------------------- \n",
      "------ Metrics for Random Forest------\n",
      "Accuracy : 0.7113125\n",
      "Precision : 0.7135548256695301\n",
      "Recall : 0.7060625\n",
      "F1 Score : 0.709788891681327\n",
      "------------------------------------------- \n",
      "------ Metrics for Gradient Boosting------\n",
      "Accuracy : 0.60053125\n",
      "Precision : 0.7290331767051118\n",
      "Recall : 0.32\n",
      "F1 Score : 0.4447726186856622\n",
      "------------------------------------------- \n",
      "------ Metrics for ADABoost------\n",
      "Accuracy : 0.64775\n",
      "Precision : 0.604666607633047\n",
      "Recall : 0.8535625\n",
      "F1 Score : 0.7078733219302338\n",
      "------------------------------------------- \n"
     ]
    }
   ],
   "source": [
    "metrics(\"Logistic Regression\", Y_test ,Y_pred_lr_TFIDF)\n",
    "metrics(\"Random Forest\", Y_test ,Y_pred_rf_TFIDF,)\n",
    "metrics(\"Gradient Boosting\", Y_test ,Y_pred_gb_TFIDF)\n",
    "metrics(\"ADABoost\", Y_test ,Y_pred_adb_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f5a6bd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Metrics for Voting Classifier Count Vectorized------\n",
      "Accuracy : 0.64775\n",
      "Precision : 0.6045925139368198\n",
      "Recall : 0.8540625\n",
      "F1 Score : 0.7079944044350034\n",
      "------------------------------------------- \n",
      "------ Metrics for Voting Classifier TFIDF------\n",
      "Accuracy : 0.64775\n",
      "Precision : 0.6045925139368198\n",
      "Recall : 0.8540625\n",
      "F1 Score : 0.7079944044350034\n",
      "------------------------------------------- \n"
     ]
    }
   ],
   "source": [
    "metrics(\"Voting Classifier Count Vectorized\", Y_test ,Y_pred_adb_CV)\n",
    "metrics(\"Voting Classifier TFIDF\", Y_test ,Y_pred_adb_CV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

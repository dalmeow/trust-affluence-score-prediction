{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46d52b1",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - Final Model Export\n",
    "   * This iPython file is just to export the selected model obtained by experimenting using the SentimentAnalysisTrails.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd5caf",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80bb02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For working with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Removing stopwords and Stemming\n",
    "import nltk\n",
    "\n",
    "# Time taken by functions\n",
    "import time\n",
    "\n",
    "# Cleaning Tweets\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba21d3b",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "* Reading the Train CSV file\n",
    "* Dropping a few useless columns\n",
    "* Mapping Positive (4) to 1 and Negative (0) to 0\n",
    "* Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5795497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Train CSV file\n",
    "\n",
    "raw_data = pd.read_csv(\"./train.csv\" , header = None , encoding = 'latin')\n",
    "\n",
    "# Dropping a few useless columns\n",
    "\n",
    "raw_data.columns = ['Y', 'A', 'B', 'C', 'D', 'X']\n",
    "raw_data = raw_data.drop(['A', 'B', 'C', 'D'], axis=1)\n",
    "\n",
    "# Mapping Positive (4) to 1 and Negative (0) to 0\n",
    "\n",
    "raw_data['Y'] = raw_data['Y'].map({4 : 1 , 0 : 0})\n",
    "raw_data['Y'].value_counts()\n",
    "\n",
    "# Shuffling the data\n",
    "\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52e12b",
   "metadata": {},
   "source": [
    "## Preprocessing the Tweets\n",
    "* Cleaning the Tweets\n",
    "    * Lowercasing\n",
    "    * Removing all the urls, user tags, hashtags and some punctuations.\n",
    "    * Stemming and removing spaces\n",
    "* Train-Test Split\n",
    "* Removing very rare words ( <= 60 occurences ).\n",
    "* Splitting into Targets and Features.\n",
    "* Count Vectorizing and TF-IDF Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97379eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767.2241866588593\n"
     ]
    }
   ],
   "source": [
    "# Declaring the function\n",
    "def process_text(text):\n",
    "\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replacing all the urls\n",
    "    text = re.sub('(?i)\\\\b((?:https?://|www\\\\d{0,3}[.]|[a-z0-9.\\\\-]+[.][a-z]{2,4}/)(?:[^\\\\s()<>]+|\\\\(([^\\\\s()<>]+|(\\\\([^\\\\s()<>]+\\\\)))*\\\\))+(?:\\\\(([^\\\\s()<>]+|(\\\\([^\\\\s()<>]+\\\\)))*\\\\)|[^\\\\s`!()\\\\[\\\\]{};:\\'\\\\\".,<>?\\xc2\\xab\\xc2\\xbb\\xe2\\x80\\x9c\\xe2\\x80\\x9d\\xe2\\x80\\x98\\xe2\\x80\\x99]))'\n",
    "               , '', text)\n",
    "\n",
    "    # Replacing all user tags\n",
    "    text = re.sub(r\"@[^\\s]+\", '', text)\n",
    "\n",
    "    # Replacing all hashtags\n",
    "    text = re.sub(r\"#[^\\s]+\", '', text)\n",
    "\n",
    "    # Remove some punctuations\n",
    "    text = re.sub(r\"[!?,'\\\"*)@#%(&$_.^-]\", '', text)\n",
    "\n",
    "    # Splitting on spaces\n",
    "    text = text.split(' ')\n",
    "\n",
    "    # Stemming and removing spaces\n",
    "    stemmer_ps = nltk.stem.PorterStemmer()  \n",
    "    text = [stemmer_ps.stem(word) for word in text if len(word)]\n",
    "\n",
    "    return text\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "raw_data['X'] = raw_data['X'].map(process_text)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a931ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train, Test = train_test_split(raw_data , stratify = raw_data[\"Y\"], test_size=0.02)\n",
    "\n",
    "Train = Train.to_numpy()\n",
    "Test = Test.to_numpy()\n",
    "\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d17af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.769980430603027\n"
     ]
    }
   ],
   "source": [
    "# Creating the word frequency dictionary to remove very rare words.\n",
    "wordfreq = {}\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for _ , text in Train:\n",
    "    for w in text:\n",
    "        if w not in wordfreq:\n",
    "            wordfreq[w] = 1\n",
    "    else:\n",
    "        wordfreq[w] += 1\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b922ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398190 2441 99.39071115315589 0.6092888468441034\n"
     ]
    }
   ],
   "source": [
    "# Getting words that have occured more than 75 times in tweets\n",
    "\n",
    "words = [word for word in wordfreq if wordfreq[word] <= 60]\n",
    "low_words = len(words)\n",
    "total_words = len(wordfreq)\n",
    "print(low_words , total_words - low_words , (low_words / total_words * 100) , ((total_words - low_words) / total_words * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad92913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the low occurence words from the wordfreq dictionaries\n",
    "\n",
    "for word in words:\n",
    "    if word in wordfreq: \n",
    "        del wordfreq[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110ca44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.897157192230225\n"
     ]
    }
   ],
   "source": [
    "# Removing the low occurence words from the tweets\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(Train)):\n",
    "    Train[i][1] = [ word for word in Train[i][1] if word in wordfreq]\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7434c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.18176007270813\n"
     ]
    }
   ],
   "source": [
    "# Joining for Vectorizer\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(Train)):\n",
    "    Train[i][1] = \" \".join(Train[i][1])\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4cac630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the targets and labels for train\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(len(Train)):\n",
    "    if ( len(Train[i][1]) > 2 ):\n",
    "        X.append(Train[i][1])\n",
    "        Y.append(Train[i][0])\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6cca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectoritzing \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_CV = CountVectorizer()\n",
    "\n",
    "X_CV = vectorizer_CV.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a736d",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f716b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the models\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fc87a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the models \n",
    "\n",
    "# Logistic Regression\n",
    "lr_CV = LogisticRegression(random_state=0 , max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bddf09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression started!\n",
      "Logistic regression ended in 107.57787108421326 secs\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression started!\")\n",
    "start = time.time()\n",
    "clf_lr_CV = lr_CV.fit(X_CV, Y)\n",
    "end = time.time()\n",
    "print(\"Logistic regression ended in \" + str(end - start) + \" secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307551d",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17dd710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the test set\n",
    "\n",
    "for i in range(len(Test)):\n",
    "    Test[i][1] = [ word for word in Test[i][1] if word in wordfreq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fa36420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the targets and labels for train\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for i in range(len(Test)):\n",
    "    X_test.append(\" \".join(Test[i][1]))\n",
    "    Y_test.append(Test[i][0])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f9db70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing\n",
    "\n",
    "X_CV_test = vectorizer_CV.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c18e2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting\n",
    "\n",
    "Y_pred_lr_CV = clf_lr_CV.predict(X_CV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b024be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a685892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics (model_name , Y_test , Y_pred ):\n",
    "    print(\"------ Metrics for \" + model_name + \"------\")\n",
    "    print(\"Accuracy : \" + str(accuracy_score(Y_test, Y_pred)))\n",
    "    print(\"Precision : \" + str(precision_score(Y_test, Y_pred)))\n",
    "    print(\"Recall : \" + str(recall_score(Y_test, Y_pred)))\n",
    "    print(\"F1 Score : \" + str(f1_score(Y_test, Y_pred)))\n",
    "    print(\"------------------------------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ca46e2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Metrics for Logistic Regression------\n",
      "Accuracy : 0.77703125\n",
      "Precision : 0.7626978012208855\n",
      "Recall : 0.8043125\n",
      "F1 Score : 0.7829525750616008\n",
      "------------------------------------------- \n"
     ]
    }
   ],
   "source": [
    "metrics(\"Logistic Regression\", Y_test ,Y_pred_lr_CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e7e5d",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a836e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vectorizer_CV.mdl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(clf_lr_CV, \"Logistic_Regression.mdl\")\n",
    "joblib.dump(vectorizer_CV, \"Vectorizer_CV.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd494664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Metrics for Logistic Regression Loaded------\n",
      "Accuracy : 0.77703125\n",
      "Precision : 0.7626978012208855\n",
      "Recall : 0.8043125\n",
      "F1 Score : 0.7829525750616008\n",
      "------------------------------------------- \n"
     ]
    }
   ],
   "source": [
    "loaded_model = joblib.load(\"Logistic_Regression.mdl\")\n",
    "result = loaded_model.predict(X_CV_test)\n",
    "\n",
    "metrics(\"Logistic Regression Loaded\", Y_test ,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd47fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_object = json.dumps(wordfreq, indent = 4)\n",
    "with open(\"wordfreq.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08a5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
